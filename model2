
import pandas as pd
import numpy as np
from sklearn import tree
from sklearn import model_selection
from sklearn.metrics import accuracy_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, mutual_info_classif
from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import mutual_info_classif
import numpy as np
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import confusion_matrix
from sklearn.feature_selection import RFE
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
from sklearn.metrics import f1_score
from sklearn.metrics import classification_report
from sklearn.linear_model import LogisticRegression
from sklearn import svm
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
#
#
# The decision tree classifier produced excellent results for this dataset. However despite several methods being used to 
# avoid overfitting and appearing not to overfit from the data, it's always a possibility with a decision tree. 
# 
# Random forests, xgboost, SVMs and neural networks are all candidates for the new classifier method to be used. Naive
# Bayes classifiers are not considered as the data is small and is trained extremely quickly on a modern computer. 
#
# It was noticed that all >300 features can be reduced to a mere 10 and human interpretability is a real possibility, the
# number of samples is also relatively small. 
#
# This rules out neural networks - which are typically used when there are large amounts of data and there is complicated 
# relationships between the data. Human interpretability is very difficult. 
# 
# Random forests and xgboost would be more suitable considering the number of samples is relatively small, they also 
# help prevent overfitting, however they complicate the structure already given of the decision tree.
#
# SVMs (Support Vector Machines) run very well on data with small amounts of samples and are also very interpretable for 
# humans, they also are not prone to overfitting. SVMs work well with low dimensional and high dimensional data, their 
# major flaw is that they don't scale well with large amounts of sample data, which is not a problem here. While 
# interpretability is harder than decision trees it is easier than neural networks and other methods. Another aspect of 
# SVMs is it is important to tune them correctly. Preprocessing techniques such as scaling are also required for SVMs.
#
# The decision was therefore made to use a SVM. 
#
 
# Load the data from a CSV file
df = p1
df_test = p2

# Drop all columns that have 1 NaN in them.
df = df.dropna(axis=1)
df = df[~df.isin([np.nan, np.inf, -np.inf]).any(1)]

df_test = df_test.dropna(axis=1)
df_test = df_test[~df_test.isin([np.nan, np.inf, -np.inf]).any(1)]


# Split the data into features (X) and target (y)
X = df.drop(['target'], axis=1).values
y = df['target'].values

# We need to pick up the names of the features here as they will be used later. 
X_features = df.drop(['target'], axis=1).columns

X_test = df_test.drop(['target'], axis=1).values
y_test = df_test['target'].values


# Feature scaling
scaler = StandardScaler()
X = scaler.fit_transform(X)
X_test = scaler.transform(X_test)

# Feature selection
selector = SelectKBest(mutual_info_classif, k=40)
X_new = selector.fit_transform(X, y)
X_new_test = selector.transform(X_test) # note use of selector.transform instead of selector.fit_transform

# Now using the indices that were extracted from the SelectKBest update the feature list.
X_features = X_features[selector.get_support(indices=True)].tolist()


# Apply Recursive Feature Elimination (RFE) for further feature reduction
estimator = svm.SVC(kernel='linear')
# NB: the default estimator did not succeed in getting the test data to 1.0 though it got very close.
# In a sense these feature reducing techniques are other methods being used. 
# However only logistic regression has been used to create the model and producing coefficience for human interpretation. 
selector = RFE(estimator, n_features_to_select=10, step=1)
X_new = selector.fit_transform(X_new, y)
X_new_test = selector.transform(X_new_test) # using selector.transform instead of selector.fit_transform

feature_index=[]
for num, i in enumerate(selector.get_support(), start=0):
    if i == True:
        feature_index.append(int(num))

X_features = np.array(X_features)
new_array = X_features[np.array(feature_index)]
new_feature_list = new_array.tolist()
print(new_feature_list)


# Hyperparameter tuning 
# At first a param grid of C = 0.1, 1, 10 was used and this resulted in accuracy of:
# Mean test score with cross-validation: 0.957544757033248
# Accuracy against test data 0.9862745098039216
# with C = 10 being chosen by the search. 
# Due to the much better results found in the decision tree it was suspected the algorithm was underfitting
# and so C = 10, 100 or 1000 were now searched. 
# Mean test score with cross-validation: 0.9790281329923275
# Accuracy against test data 1.0
#
# Note: The follwoing warning given iterations reached limit is erroneous as the data has already been scaled 
# and increasing the max_iter with eg. LogisticRegression(max_iter=10000) only results in longer training time 
# without improving accuracy. 
# 
# STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.
#
# Increase the number of iterations (max_iter) or scale the data as shown in:
# https://scikit-learn.org/stable/modules/preprocessing.html
#
#
#
 
param_grid = {'C': [10,100,1000,10000],
'penalty': ['l1', 'l2']}
model = LogisticRegression()
grid_search = GridSearchCV(model, param_grid, cv=5)
grid_search.fit(X_new, y)
best_params = grid_search.best_params_
print(best_params)


print(best_params)

if best_params is None:
# Set default hyperparameters to use when best_params is None
        best_params = {'C': 10000, 'penalty': 'l2'}


model = LogisticRegression(**best_params)
scores = cross_val_score(model, X_new, y, cv=5)
mean_test_score = np.mean(scores)
print('Mean test score with cross-validation:', mean_test_score)


model.fit(X_new, y) # fit the model using the entire training set

y_pred = model.predict(X_new_test)

# Evaluate the performance of the model by comparing the predicted target variables with the actual target variables in p2
accuracy = accuracy_score(y_test, y_pred)

print("Accuracy against test data",accuracy)





# create confusion matrix
cm = confusion_matrix(y_test, y_pred)

# create heatmap
sns.heatmap(cm, annot=True, cmap=plt.cm.Blues, xticklabels=df['target'].unique(),
            yticklabels=df['target'].unique())

# add labels and title
plt.xlabel('Predicted label')
plt.ylabel('True label')
plt.title('Confusion Matrix')

# Calculate the accuracy score
accuracy_test = accuracy_score(y_test, y_pred)
print('Accuracy score on test dataset:', accuracy_test)
 
# Get the coefficients of the fitted model
coefs = model.coef_[0]

# Plot the coefficients
plt.figure(figsize=(8,6))
plt.bar(range(len(coefs)), coefs)
plt.title("Coefficients of Logistic Regression Model")
plt.xlabel("Feature")
plt.ylabel("Coefficient value")
plt.xticks(range(len(coefs)),new_feature_list, rotation=90)
plt.show()

print(classification_report(y_test, y_pred))
  
